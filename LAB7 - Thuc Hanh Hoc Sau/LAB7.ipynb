{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Downloading package gutenberg to\n",
      "        C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "      Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_eng Averaged Perceptron Tagger (JSON)\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] averaged_perceptron_tagger_rus Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] bcp47............... BCP-47 Language Tags\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [*] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_ne_chunker_tab ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger_tab Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [ ] punkt............... Punkt Tokenizer Models\n",
      "  [ ] punkt_tab........... Punkt Tokenizer Models\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n",
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
      "  [ ] stopwords........... Stopwords Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] tagsets_json........ Help on Tagsets (JSON)\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet2021......... Open English Wordnet 2021\n",
      "  [ ] wordnet2022......... Open English Wordnet 2022\n",
      "  [ ] wordnet31........... Wordnet 3.1\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [P] all-corpora......... All the corpora\n",
      "  [P] all-nltk............ All packages available on nltk_data gh-pages\n",
      "                           branch\n",
      "  [P] all................. All packages\n",
      "  [P] book................ Everything used in the NLTK Book\n",
      "  [P] popular............. Popular packages\n",
      "  [ ] tests............... Packages for running tests\n",
      "  [ ] third-party......... Third-party data packages\n",
      "\n",
      "([*] marks installed packages; [P] marks partially installed collections)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download_shell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gutenberg filé : ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "qb = nltk.corpus.gutenberg\n",
    "print(\"Gutenberg filé :\", qb.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "macbeth = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23140"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(macbeth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'The',\n",
       " 'Tragedie',\n",
       " 'of',\n",
       " 'Macbeth',\n",
       " 'by',\n",
       " 'William',\n",
       " 'Shakespeare',\n",
       " '1603',\n",
       " ']']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth [:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['[',\n",
       "  'The',\n",
       "  'Tragedie',\n",
       "  'of',\n",
       "  'Macbeth',\n",
       "  'by',\n",
       "  'William',\n",
       "  'Shakespeare',\n",
       "  '1603',\n",
       "  ']'],\n",
       " ['Actus', 'Primus', '.'],\n",
       " ['Scoena', 'Prima', '.'],\n",
       " ['Thunder', 'and', 'Lightning', '.'],\n",
       " ['Enter', 'three', 'Witches', '.']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "macbeth_sents = nltk.corpus.gutenberg.sents('shakespeare-macbeth.txt')\n",
    "macbeth_sents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 3 of 3 matches:\n",
      "nts with Dishes and Seruice ouer the Stage . Then enter Macbeth Macb . If it we\n",
      "with mans Act , Threatens his bloody Stage : byth ' Clock ' tis Day , And yet d\n",
      " struts and frets his houre vpon the Stage , And then is heard no more . It is \n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(macbeth)\n",
    "text.concordance('Stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_. bloody_: the_,\n"
     ]
    }
   ],
   "source": [
    "text.common_contexts(['Stage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day time face warre ayre king bleeding man reuolt serieant like\n",
      "knowledge broyle shew head spring heeles hare thane skie\n"
     ]
    }
   ],
   "source": [
    "text.similar('Stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 1962),\n",
       " ('.', 1235),\n",
       " (\"'\", 637),\n",
       " ('the', 531),\n",
       " (':', 477),\n",
       " ('and', 376),\n",
       " ('I', 333),\n",
       " ('of', 315),\n",
       " ('to', 311),\n",
       " ('?', 241)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = nltk.FreqDist(macbeth)\n",
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['herself',\n",
       " 'by',\n",
       " 'from',\n",
       " \"doesn't\",\n",
       " 'were',\n",
       " 'it',\n",
       " 'did',\n",
       " \"should've\",\n",
       " \"they'd\",\n",
       " 'most']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = set(nltk.corpus.stopwords.words('english'))\n",
    "print(len(sw))\n",
    "list(sw) [:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14946"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth_filtered = [w for w in macbeth if w.lower() not in sw]\n",
    "len(macbeth_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 1962),\n",
       " ('.', 1235),\n",
       " (\"'\", 637),\n",
       " (':', 477),\n",
       " ('?', 241),\n",
       " ('Macb', 137),\n",
       " ('haue', 117),\n",
       " ('-', 100),\n",
       " ('Enter', 80),\n",
       " ('thou', 63)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = nltk.FreqDist(macbeth_filtered)\n",
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 1962),\n",
       " ('.', 1235),\n",
       " (\"'\", 637),\n",
       " (':', 477),\n",
       " ('?', 241),\n",
       " ('Macb', 137),\n",
       " ('haue', 117),\n",
       " ('-', 100),\n",
       " ('Enter', 80),\n",
       " ('thou', 63)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string \n",
    "punctuuation = set(string.punctuation)\n",
    "macbeth_filtered2 = [w.lower() for w in macbeth if w.lower() not in sw and w.lower() not in punctuuation]\n",
    "pd = nltk.FreqDist(macbeth_filtered2)\n",
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Assassination',\n",
       " 'Chamberlaines',\n",
       " 'Distinguishes',\n",
       " 'Gallowgrosses',\n",
       " 'Metaphysicall',\n",
       " 'Northumberland',\n",
       " 'Voluptuousnesse',\n",
       " 'commendations',\n",
       " 'multitudinous',\n",
       " 'supernaturall',\n",
       " 'vnaccompanied']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_words = [w for w in  macbeth if len(w)> 12]\n",
    "sorted(long_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Auaricious',\n",
       " 'Gracious',\n",
       " 'Industrious',\n",
       " 'Iudicious',\n",
       " 'Luxurious',\n",
       " 'Malicious',\n",
       " 'Obliuious',\n",
       " 'Pious',\n",
       " 'Rebellious',\n",
       " 'compunctious',\n",
       " 'furious',\n",
       " 'gracious',\n",
       " 'pernicious',\n",
       " 'pernitious',\n",
       " 'pious',\n",
       " 'precious',\n",
       " 'rebellious',\n",
       " 'sacrilegious',\n",
       " 'serious',\n",
       " 'spacious',\n",
       " 'tedious']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ious_words = [w for w in macbeth if 'ious' in w]\n",
    "ious_words = set(ious_words)\n",
    "sorted(ious_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('enter', 'macbeth'), 16),\n",
       " (('exeunt', 'scena'), 15),\n",
       " (('thane', 'cawdor'), 13),\n",
       " (('knock', 'knock'), 10),\n",
       " (('st', 'thou'), 9),\n",
       " (('thou', 'art'), 9),\n",
       " (('lord', 'macb'), 9),\n",
       " (('haue', 'done'), 8),\n",
       " (('macb', 'haue'), 8),\n",
       " (('good', 'lord'), 8),\n",
       " (('let', 'vs'), 7),\n",
       " (('enter', 'lady'), 7),\n",
       " (('wee', 'l'), 7),\n",
       " (('would', 'st'), 6),\n",
       " (('macbeth', 'macb'), 6)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgrms = nltk.FreqDist(nltk.bigrams(macbeth_filtered2))\n",
    "bgrms.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('knock', 'knock', 'knock'), 6),\n",
       " (('enter', 'macbeth', 'macb'), 5),\n",
       " (('enter', 'three', 'witches'), 4),\n",
       " (('exeunt', 'scena', 'secunda'), 4),\n",
       " (('good', 'lord', 'macb'), 4),\n",
       " (('three', 'witches', '1'), 3),\n",
       " (('exeunt', 'scena', 'tertia'), 3),\n",
       " (('thunder', 'enter', 'three'), 3),\n",
       " (('exeunt', 'scena', 'quarta'), 3),\n",
       " (('scena', 'prima', 'enter'), 3)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgrms = nltk.FreqDist(nltk.trigrams (macbeth_filtered2))\n",
    "tgrms.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*** START OF THE PROJECT GUTENBERG EBOOK 2554 ***\\n\\n\\n\\n\\nCRIME AND PUNISHMENT\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response =  request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "raw [:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*** START OF THE PROJECT GUTENBERG EBOOK 2554 ***\\n\\n\\n\\n\\nCRIME AND PUNISHMENT\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response =  request.urlopen(url)\n",
    "raw = response.read().decode('utf-8 -sig')\n",
    "raw [:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['*',\n",
       " '*',\n",
       " '*',\n",
       " 'START',\n",
       " 'OF',\n",
       " 'THE',\n",
       " 'PROJECT',\n",
       " 'GUTENBERG',\n",
       " 'EBOOK',\n",
       " '2554',\n",
       " '*',\n",
       " '*']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize (raw)\n",
    "webtext = nltk.Text (tokens)\n",
    "webtext[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose.dtd\">\\r\\n<html>\\r\\n<hea'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "html[:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Thiên đường nghỉ dưỡng Hồ Tràm\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Zalo/WhatApps: (+84) 90336 5555 - (+84) 907936622\n",
      " thecottagehotramhills@gmail.com \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Trang chủ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Giới thiệu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Villa\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bảng giá Villa\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Quy định\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Điểm tham quan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tin tức\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Liên hệ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Cottage Hồ Tràm Hills\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Trải Nghiệm Cuộc Sống Muôn Màu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Cottage Hồ Tràm Hills\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Nơi tình yêu bắt đầu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Thiên đường nghỉ dưỡng Hồ Tràm \n",
      "\n",
      "Thiên đường nghỉ dưỡng Hồ Tràm\n",
      "Hồ Tràm sở hữu hệ sinh thái đa dạng với 11.000 ha rừng nguyên sinh xanh mát, với bãi biển Hồ Tràm trải dài. Cùng các khu nghỉ dưỡng, khách sạn, Villa biệt thự cao cấp như NovaWorld Ho Tram, The Grand Ho Tram Strip, Sanctuary, Melia Ho Tram, Carmelina... đã và đang góp phần tạo nên một thiên đường nghỉ dưỡng Hồ Tràm đẳng cấp quốc tế.\n",
      "Với sự kết hợp hoàn hảo giữa thiên nhiên hoang sơ và tiện nghi hiện đại, biển Hồ Tràm xứng đáng là điểm đến lý tưởng cho những ai muốn tận hưởng chuyến du lịch biển trọn vẹn, khám phá vẻ đẹp tự nhiên và trải nghiệm những dịch vụ đẳng cấp.\n",
      "Villa Biệt thự The Cottage Hồ Tràm sang trọng, tiện nghi và riêng tư\n",
      "Villa biệt thự toạ lạc biệt lập giữa thiên nhiên Hồ Tràm hoang sơ, ngay sát sân golf đẳng cấp The Bluffs và chỉ cách bãi biển Hồ Tràm vài phút đi bộ.\n",
      "Với diện tích hơn 1500m2, Villa biệt thự tại đây đều được thiết kế tinh tế, hài hòa với thiên nhiên, mang đến cảm giác ấm cúng và gần gũi. Không gian sinh hoạt được bố trí rộng rãi, thoáng mát, với đầy đủ các tiện nghi hiện đại như 2 bể Jacuzzi, khu vườn xanh mát, phòng bếp đầy đủ dụng cụ, phòng khách sang trọng... Du khách có thể thoải mái thư giãn bên bể sục, tổ chức những bữa tiệc BBQ ngoài trời, hoặc đơn giản chỉ là tận hưởng không khí trong lành, lắng nghe tiếng chim hót.\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Các phòng Villa\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Phòng ngủ master \n",
      "\n",
      "Chi tiết\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Phòng ngủ\n",
      "\n",
      "Chi tiết\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Phòng khách \n",
      "\n",
      "Chi tiết\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sân vườn\n",
      "\n",
      "Chi tiết\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bếp \n",
      "\n",
      "Chi tiết\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Nhà gỗ\n",
      "\n",
      "Chi tiết\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tin Nổi bật\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "02/03/2025\n",
      "Thuê Villa Hồ Tràm Giá Rẻ Cho Nhóm 12 Người\n",
      "Thuê villa Hồ Tràm giá rẻ cho nhóm 12 người? The Cottage Hồ Tràm Hills là lựa chọn hoàn hảo với tiện nghi đầy đủ, giá hấp dẫn!\n",
      "Chi tiết\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "28/02/2025\n",
      "Thuê Villa Hồ Tràm Gần Suối Nước Nóng Bình Châu\n",
      "Thuê villa Hồ Tràm gần suối nước nóng Bình Châu, tận hưởng kỳ nghỉ lý tưởng với biển xanh, khoáng nóng thư giãn và không gian nghỉ dưỡng sang trọng.\n",
      "Chi tiết\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sự kiện Nổi bật\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "02Tháng 03\n",
      "\n",
      "Thuê Villa Hồ Tràm Giá Rẻ Cho Nhóm 12 Người\n",
      "\n",
      "\n",
      "\n",
      "28Tháng 02\n",
      "\n",
      "Thuê Villa Hồ Tràm Gần Suối Nước Nóng Bình Châu\n",
      "\n",
      "\n",
      "\n",
      "27Tháng 02\n",
      "\n",
      "Thuê Biệt Thự Hồ Tràm Giá Bao Nhiêu? Cập Nhật Mới Nhất!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Video\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Liên hệ\n",
      "\n",
      "The Cottage Hồ Tràm Hills\n",
      "Tổ 8, ĐT 328, ấp Hồ Tràm, xã Phước Thuận, huyện Xuyên Mộc, tỉnh Bà Rịa - Vũng Tàu\n",
      "\n",
      "Zalo/WhatApps: (+84) 90336 5555 - (+84) 907936622\n",
      "\n",
      "thecottagehotramhills@gmail.com \n",
      "thecottagehotram.com\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Villa\n",
      "\n",
      "Phòng ngủ master \n",
      "Phòng ngủ\n",
      "Phòng khách \n",
      "Sân vườn\n",
      "Bếp \n",
      "Nhà gỗ\n",
      "\n",
      "\n",
      "\n",
      "Trang chủ\n",
      "\n",
      "Giới thiệu\n",
      "Bảng giá Villa\n",
      "Quy định\n",
      "Điểm tham quan\n",
      "Tin tức\n",
      "Liên hệ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "© 2025 THECOTTAGEHOTRAM.COM. All rights reserved\n",
      "\n",
      "\n",
      "\n",
      "Zalo/WhatApps: (+84) 90336 5555 - (+84) 907936622\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Lấy nội dung từ trang web\n",
    "url = \"https://thecottagehotram.com/\"\n",
    "response = requests.get(url)\n",
    "html = response.text  # Gán nội dung HTML vào biến\n",
    "\n",
    "# Phân tích HTML với lxml\n",
    "raw = BeautifulSoup(html, \"html.parser\").get_text()\n",
    "print(raw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['whenever', 'studio', 'executives', 'try', 'to', 'combine', 'the', 'successful', 'formula', 'of', 'two', 'blockbuster', 'films', ',', 'the', 'end', 'result', 'is', 'always', 'a', 'mess', 'of', 'a', 'script', ',', 'acting', ',', 'directing', ',', 'and', 'generally', 'a', 'waste', 'of', 'time', 'for', 'any', 'audience', 'with', 'an', 'average', 'iq', 'above', '40', '.', 'the', 'latest', 'movie', 'morass', 'from', 'hollywood', ',', 'the', 'crew', ',', 'appeared', 'equally', 'destined', 'to', 'fail', ',', 'a', 'strange', 'combination', 'of', 'grumpy', 'old', 'men', 'and', 'goodfellas', 'tossed', 'together', 'with', 'the', 'likes', 'of', 'richard', 'dreyfuss', 'and', 'burt', 'reynolds', 'helming', 'the', 'ship', '.', 'the', 'scary', 'thing', 'is', 'that', 'it', \"'\", 's', 'actually', 'entertaining', 'and', 'a', 'breath', 'of', 'fresh', 'air', 'in', 'this', 'otherwise', 'stale', 'month', '.', 'the', 'crew', 'works', 'for', 'several', 'reasons', '.', 'the', 'clever', 'script', 'is', 'reminiscent', 'of', 'an', 'old', 'billy', 'wilder', 'movie', ',', 'following', 'four', '\"', 'past', 'their', 'prime', '\"', 'wiseguys', 'from', 'jersey', 'who', 'now', 'live', 'in', 'the', 'raj', 'mahal', 'apartment', 'house', 'in', 'miami', 'beach', '.', 'the', 'wiseguys', 'find', 'themselves', 'being', 'evicted', 'from', 'their', '\"', 'golden', 'paradise', '\"', 'by', 'greedy', 'landlords', 'bent', 'on', 'raising', 'rents', 'for', 'new', 'beach', 'bunnies', 'and', 'boys', 'looking', 'for', 'beachfront', 'property', '.', 'the', 'four', 'mobsters', ',', 'bobby', 'bartellemeo', '(', 'richard', 'dreyfuss', ')', ',', 'joey', '\"', 'bats', '\"', 'pistella', '(', 'burt', 'reynolds', ')', ',', 'mike', '\"', 'the', 'brick', '\"', 'donatelli', '(', 'dan', 'hedaya', ')', ',', 'and', 'tony', '\"', 'mouth', '\"', 'donato', '(', 'seymour', 'cassel', ')', 'decide', 'to', 'hatch', 'a', 'scheme', 'to', 'plant', 'there', 'a', 'dead', 'body', 'heisted', 'from', 'the', 'morgue', 'in', 'order', 'to', 'drive', 'out', 'the', 'new', 'tenants', 'and', 'keep', 'their', 'home', '.', 'this', '\"', 'simple', 'plan', '\"', 'suddenly', 'goes', 'screwy', ',', 'of', 'course', ',', 'and', 'the', 'boys', 'become', 'involved', 'with', 'a', 'stripper', 'named', 'ferris', '(', 'jennifer', 'tilly', ')', 'who', 'wants', 'her', 'stepmother', 'killed', ',', 'a', 'paranoid', 'latin', 'drug', 'lord', 'who', \"'\", 's', 'convinced', 'a', 'mysterious', 'rival', 'is', 'out', 'to', 'get', 'him', ',', 'and', 'a', 'rat', 'with', 'its', 'tail', 'on', 'fire', '.', 'the', 'crew', 'works', 'as', 'a', 'strong', 'comedic', 'vehicle', 'driven', 'by', 'a', 'great', 'ensemble', 'cast', 'of', 'talented', 'character', 'actors', 'and', 'subtle', 'leading', 'men', '.', 'instead', 'of', 'concentrating', 'on', 'bobby', \"'\", 's', 'search', 'for', 'his', 'daughter', ',', 'the', 'film', 'gives', 'equal', 'screen', 'time', 'to', 'all', 'four', 'wiseguys', ',', 'balancing', 'the', 'production', '.', 'the', 'acting', 'talents', 'of', 'such', 'screen', 'veterans', 'as', 'dan', 'hedaya', ',', 'seymour', 'cassel', ',', 'and', 'the', 'great', ',', 'great', 'burt', 'reynolds', 'are', 'brought', 'out', 'by', 'each', 'character', \"'\", 's', 'uniqueness', 'and', 'synergy', 'within', 'the', 'wiseguy', 'circle', '.', 'a', 'strong', 'supporting', 'cast', 'including', 'jeremy', 'piven', 'as', 'a', 'philandering', 'boyfriend', '/', 'cop', '/', 'partner', 'and', 'miguel', 'sandoval', 'as', 'the', 'paranoid', 'drug', 'lord', 'provide', 'the', 'most', 'laughs', '.', 'the', 'film', 'was', 'produced', 'by', 'barry', 'sonnenfeld', 'and', 'carries', 'a', 'hint', 'get', 'shorty', 'with', 'its', 'energetic', 'camera', 'work', 'by', 'michael', 'dinner', ',', 'emmy', 'award', '-', 'winning', 'director', 'of', 'the', 'wonder', 'years', '.', 'the', 'quick', 'script', 'written', 'by', 'barry', 'fanaro', ',', 'screenwriter', 'of', 'kingpin', ',', 'is', 'carried', 'by', 'plenty', 'of', 'subtly', 'dry', 'humor', 'and', 'wit', '.', 'the', 'crew', 'is', 'a', 'prime', 'example', 'of', 'how', 'simple', 'and', 'dry', '-', 'witted', 'a', 'comedy', 'can', 'get', 'these', 'days', '.', 'the', 'great', 'thing', 'is', 'that', 'this', 'comedy', 'does', 'not', 'need', 'to', 'be', 'punctuated', 'by', 'bodily', 'fluids', ',', 'phallus', 'symbols', 'in', 'the', 'head', ',', 'or', 'grandmothers', 'giving', 'head', 'in', 'a', 'spa', '.', 'in', 'the', 'steady', 'flow', 'of', '\"', 'extreme', '\"', 'comedy', 'these', 'days', 'flowing', 'from', 'hollywood', 'like', 'a', 'broken', 'water', 'main', ',', 'it', \"'\", 's', 'the', 'elder', 'statesmen', 'like', 'dreyfuss', ',', 'reynolds', ',', 'and', 'hedaya', 'that', 'can', 'carry', 'a', 'comic', 'arc', 'with', 'only', 'a', 'few', 'weapons', 'of', 'choice', '-', 'a', 'decent', 'script', 'and', 'good', 'acting', '.'], 'pos'), (['capsule', ':', 'where', 'are', 'you', 'tonight', ',', 'leni', 'rienfenstal', '?', 'starship', 'troopers', 'is', 'an', 'expensive', ',', 'hateful', 'and', 'unenjoyable', 'piece', 'of', 'violent', 'pornography', '--', 'and', 'bad', 'pornography', ',', 'at', 'that', '.', 'it', 'is', 'not', 'good', 'cinema', ',', 'not', 'good', 'storytelling', ',', 'and', 'not', 'even', 'stupid', 'fun', ':', 'it', \"'\", 's', 'so', 'cynically', ',', 'calculatedly', 'bone', '-', 'headed', 'that', 'even', 'the', 'least', 'demanding', 'members', 'of', 'the', 'audience', 'i', 'was', 'with', 'were', 'alternately', 'bored', 'and', 'revulsed', '.', 'it', \"'\", 's', 'one', 'of', 'the', 'worst', 'movies', 'i', \"'\", 've', 'seen', 'in', 'a', 'long', 'time', '.', 'why', 'am', 'i', 'calling', 'this', 'movie', 'pornography', '?', 'pornography', ',', 'in', 'the', 'abstract', ',', 'is', 'anything', 'which', 'is', 'calculated', 'to', 'appeal', 'to', 'the', 'baser', 'instincts', '.', 'i', 'remember', 'reading', 'a', 'review', 'of', 'full', 'metal', 'jacket', 'which', 'described', 'the', 'climact', 'moment', 'at', 'the', 'end', 'of', 'the', 'movie', ',', 'where', 'private', 'joker', 'has', 'to', 'shoot', 'the', 'downed', 'vc', 'sniper', ',', 'as', '\"', 'a', 'near', '-', 'pornographic', 'eternity', '\"', '.', 'i', 'didn', \"'\", 't', 'agree', 'with', 'that', 'assessment', ',', 'but', 'i', 'could', 'see', 'what', 'was', 'being', 'implied', ':', 'the', 'reviewer', 'felt', 'as', 'if', 'the', 'audience', 'was', 'being', 'incited', 'to', 'stand', 'up', 'and', 'pump', 'their', 'fists', 'and', 'shout', '\"', 'do', \"'\", 'er', ',', 'man', ',', 'do', \"'\", 'er', '!', '\"', 'that', 'sentiment', 'is', 'echoed', 'ad', 'nauseam', 'throughout', 'starship', 'troopers', '.', 'it', \"'\", 's', '*', 'literally', '*', 'like', 'a', 'giant', 'recruitment', 'film', 'that', 'has', 'gone', 'berserk', '.', 'it', 'tries', 'frantically', 'to', 'enlist', 'our', 'emotions', 'but', 'it', 'winds', 'up', 'only', 'being', 'dull', 'or', 'sickening', '.', 'the', 'film', 'takes', 'place', 'in', 'our', 'future', ',', 'when', 'earth', 'has', 'come', 'under', 'attack', 'by', 'some', 'alien', 'species', 'that', 'doesn', \"'\", 't', 'appear', 'to', 'have', 'intelligence', '.', 'to', 'counterattack', ',', 'humankind', 'does', 'the', 'single', 'stupidest', 'thing', 'imaginable', ':', 'instead', 'of', 'nuke', 'the', 'planet', 'from', 'orbit', ',', 'which', 'they', \"'\", 're', 'clearly', 'capable', 'of', ',', 'they', 'send', 'down', 'grunts', 'with', 'm', '-', '16', 'rifles', '.', 'this', 'isn', \"'\", 't', 'heinlein', ',', 'it', \"'\", 's', 'a', 'bad', 'wwii', 'movie', ',', 'one', 'where', 'all', 'the', 'most', 'obvious', 'points', 'of', 'satire', 'are', 'ever', 'poked', 'and', 'prodded', 'in', 'combat', 'scenes', 'that', 'are', 'noisy', ',', 'repetitive', ',', 'and', 'ultimately', 'tiresome', '.', '(', 'one', 'scene', 'has', 'a', 'journalist', 'on', 'a', 'battlefield', ',', 'filimg', 'soldiers', 'being', 'slaughtered', ',', 'and', 'ends', 'with', 'groaning', 'predictability', ':', 'said', 'cameraman', 'gets', 'skewered', ',', 'too', '.', ')', 'the', 'soldiers', 'and', 'their', 'commanders', 'are', 'consistently', 'idiotic', '.', 'they', 'do', 'not', 'possess', 'a', 'germ', 'of', 'tactical', 'intelligence', 'or', 'even', 'common', 'sense', '.', 'they', 'don', \"'\", 't', 'even', 'behave', 'like', 'soldiers', 'in', 'a', 'bad', '*', 'movie', '*', ',', 'and', 'therefore', 'we', 'don', \"'\", 't', 'care', 'about', 'them', '.', 'we', 'hated', 'the', 'tom', 'berenger', 'character', 'in', 'platoon', ',', 'but', 'he', '*', 'mattered', '*', ',', 'and', 'therefore', 'we', 'were', 'curious', 'to', 'learn', 'about', 'his', 'fate', '.', 'here', ',', 'we', 'don', \"'\", 't', 'even', 'hate', 'the', 'bugs', '--', 'what', \"'\", 's', 'there', 'to', 'hate', '?', 'hating', 'them', 'would', 'be', 'like', 'cursing', 'a', 'hurricaine', '.', 'if', 'there', \"'\", 's', 'anything', 'really', 'hateful', 'there', ',', 'the', 'movie', 'doesn', \"'\", 't', 'know', 'how', 'to', 'give', 'it', 'to', 'us', '.', 'the', 'screenwriter', '(', 'ed', 'neumeier', '--', 'perhaps', 'a', 'better', 'appelation', 'would', 'be', '\"', 'screen', 'typist', '\"', ')', 'and', 'director', '(', 'the', 'increasingly', 'talent', '-', 'impaired', 'paul', 'verhoeven', ')', 'have', 'not', 'found', 'any', 'way', 'to', 'make', 'the', 'characters', 'or', 'the', 'story', 'serve', 'each', 'other', '.', 'one', 'of', 'the', 'subplots', 'concerns', 'a', 'woman', 'pilot', 'who', \"'\", 's', 'great', 'at', 'getting', 'out', 'of', 'tight', 'situations', ',', 'and', 'does', 'it', 'again', 'and', 'again', 'and', 'again', '.', 'once', 'or', 'twice', 'is', 'fine', '.', 'by', 'the', 'fifth', 'or', 'sixth', 'time', ',', 'it', \"'\", 's', 'worn', 'out', 'its', 'welcome', '.', 'there', \"'\", 's', 'never', 'any', 'sense', 'that', 'these', 'people', 'are', 'really', 'thinking', 'their', 'way', 'out', 'of', 'anything', ',', 'or', 'really', 'being', 'tested', 'to', 'show', 'their', 'mettle', '.', 'also', ',', 'the', 'movie', 'is', 'irritatingly', 'selective', 'with', 'how', 'effective', 'the', 'bugs', 'are', 'to', 'earthling', 'weaponry', '.', 'if', 'a', 'bug', 'has', 'one', 'of', 'the', 'humans', 'screaming', 'in', 'its', 'grasp', ',', 'then', 'five', 'guys', 'can', 'stand', 'around', 'it', 'and', 'blast', 'away', 'on', 'full', 'auto', 'without', 'doing', 'a', 'damned', 'thing', '.', 'but', 'if', 'one', 'human', 'gets', 'cornered', ',', 'he', 'lays', 'waste', 'to', 'whole', 'platoons', 'of', 'bugs', 'with', 'one', 'clip', '.', 'uh', '-', 'huh', '.', 'the', 'very', 'worst', 'feature', 'of', 'the', 'movie', 'is', 'its', 'repulsive', 'quasi', '-', 'fascist', 'flavor', '.', 'i', 'say', '\"', 'quasi', '-', '\"', 'because', 'while', 'the', 'movie', 'uses', 'many', 'of', 'the', 'trappings', 'of', 'fascism', 'ot', 'eroticize', 'its', 'action', '--', 'the', 'gear', ',', 'the', 'uniforms', ',', 'etc', '.', '--', 'the', 'movie', 'doesn', \"'\", 't', 'have', 'the', 'nerve', '(', 'or', 'the', 'brains', ')', 'to', 'be', 'genuinely', 'fascist', ',', 'or', 'intelligent', 'about', 'the', 'subject', '.', 'the', 'bumpers', 'between', 'scenes', ',', 'which', 'are', 'apparently', 'intended', 'to', 'parody', 'wartime', 'recruitment', 'propaganda', 'are', 'propaganda', '--', 'just', 'so', 'clumsy', 'and', 'oafish', 'that', 'they', 'wind', 'up', 'making', 'the', 'bugs', 'look', 'relatively', 'innocuous', 'in', 'comparison', '.', 'like', 'the', 'rest', 'of', 'the', 'movie', '.', 'there', \"'\", 's', 'more', ',', 'i', 'suppose', ',', 'but', 'it', \"'\", 's', 'not', 'worth', 'it', '.', 'the', 'acting', 'is', 'bland', ',', 'neither', 'arsenic', 'nor', 'gravy', ';', 'the', 'music', 'disposable', ';', 'the', 'camerawork', 'turgid', '.', 'the', 'heartbreaking', 'thing', 'is', 'that', 'it', 'makes', 'independence', 'day', 'look', 'like', 'a', 'masterpiece', '.'], 'neg')]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "# Tạo danh sách các tài liệu, mỗi tài liệu là một tuple (danh sách từ, nhãn)\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Xáo trộn danh sách tài liệu\n",
    "random.shuffle(documents)\n",
    "\n",
    "# Kiểm tra kết quả\n",
    "print(documents[:2])  # In thử 2 phần tử đầu tiên\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whenever studio executives try to combine the successful formula of two blockbuster films , the end result is always a mess of a script , acting , directing , and generally a waste of time for any audience with an average iq above 40 . the latest movie morass from hollywood , the crew , appeared equally destined to fail , a strange combination of grumpy old men and goodfellas tossed together with the likes of richard dreyfuss and burt reynolds helming the ship . the scary thing is that it ' s actually entertaining and a breath of fresh air in this otherwise stale month . the crew works for several reasons . the clever script is reminiscent of an old billy wilder movie , following four \" past their prime \" wiseguys from jersey who now live in the raj mahal apartment house in miami beach . the wiseguys find themselves being evicted from their \" golden paradise \" by greedy landlords bent on raising rents for new beach bunnies and boys looking for beachfront property . the four mobsters , bobby bartellemeo ( richard dreyfuss ) , joey \" bats \" pistella ( burt reynolds ) , mike \" the brick \" donatelli ( dan hedaya ) , and tony \" mouth \" donato ( seymour cassel ) decide to hatch a scheme to plant there a dead body heisted from the morgue in order to drive out the new tenants and keep their home . this \" simple plan \" suddenly goes screwy , of course , and the boys become involved with a stripper named ferris ( jennifer tilly ) who wants her stepmother killed , a paranoid latin drug lord who ' s convinced a mysterious rival is out to get him , and a rat with its tail on fire . the crew works as a strong comedic vehicle driven by a great ensemble cast of talented character actors and subtle leading men . instead of concentrating on bobby ' s search for his daughter , the film gives equal screen time to all four wiseguys , balancing the production . the acting talents of such screen veterans as dan hedaya , seymour cassel , and the great , great burt reynolds are brought out by each character ' s uniqueness and synergy within the wiseguy circle . a strong supporting cast including jeremy piven as a philandering boyfriend / cop / partner and miguel sandoval as the paranoid drug lord provide the most laughs . the film was produced by barry sonnenfeld and carries a hint get shorty with its energetic camera work by michael dinner , emmy award - winning director of the wonder years . the quick script written by barry fanaro , screenwriter of kingpin , is carried by plenty of subtly dry humor and wit . the crew is a prime example of how simple and dry - witted a comedy can get these days . the great thing is that this comedy does not need to be punctuated by bodily fluids , phallus symbols in the head , or grandmothers giving head in a spa . in the steady flow of \" extreme \" comedy these days flowing from hollywood like a broken water main , it ' s the elder statesmen like dreyfuss , reynolds , and hedaya that can carry a comic arc with only a few weapons of choice - a decent script and good acting .\n"
     ]
    }
   ],
   "source": [
    "first_review =' '.join(documents[0] [0])\n",
    "print(first_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in reviews.words())\n",
    "word_features = list(all_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['{}'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets = [(document_features(d,word_features), c) for (d,c) in documents]\n",
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, est_set = featuresets[1500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.788\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = featuresets[1500:], featuresets[:500]  # Sửa 'est_set' thành 'test_set'\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Sử dụng đúng biến 'test_set' để đánh giá độ chính xác\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             wonderfully = True              pos : neg    =     11.9 : 1.0\n",
      "                flawless = True              pos : neg    =      9.2 : 1.0\n",
      "                    lame = True              neg : pos    =      9.2 : 1.0\n",
      "                  finest = True              pos : neg    =      8.5 : 1.0\n",
      "                   awful = True              neg : pos    =      8.5 : 1.0\n",
      "                  turkey = True              neg : pos    =      8.1 : 1.0\n",
      "                  london = True              pos : neg    =      7.9 : 1.0\n",
      "                   grade = True              neg : pos    =      7.5 : 1.0\n",
      "                     iii = True              neg : pos    =      7.5 : 1.0\n",
      "                  flawed = True              pos : neg    =      7.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bài tập về nhà"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all-corpora'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all-corpora\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all-corpora')\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Liệt kê các tên tài liệu trong corpus Gutenberg\n",
    "print(gutenberg.fileids())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['albanian', 'arabic', 'azerbaijani', 'basque', 'belarusian', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Danh sách các ngôn ngữ có trong stopwords\n",
    "print(stopwords.fileids())\n",
    "\n",
    "# Hiển thị stopwords của tiếng Anh\n",
    "print(stopwords.words('english')[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords tiếng Anh: ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n",
      "Stopwords tiếng Pháp: ['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle']\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra stopwords của tiếng Anh và tiếng Pháp\n",
    "print(\"Stopwords tiếng Anh:\", stopwords.words('english')[:10])\n",
    "print(\"Stopwords tiếng Pháp:\", stopwords.words('french')[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered text: simple example remove stopwords text .\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"This is a simple example to remove stopwords from a text.\"\n",
    "words = word_tokenize(text.lower())\n",
    "\n",
    "filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "print(\"Filtered text:\", \" \".join(filtered_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered text: simple example remove stopwords text .\n"
     ]
    }
   ],
   "source": [
    "# Xóa một số stopword khỏi danh sách stopwords của tiếng Anh\n",
    "custom_stopwords = set(stopwords.words('english'))\n",
    "custom_stopwords.discard('not')  # Giữ lại từ \"not\"\n",
    "\n",
    "filtered_words = [word for word in words if word not in custom_stopwords]\n",
    "print(\"Filtered text:\", \" \".join(filtered_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Định nghĩa: a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
      "Ví dụ: ['he needs a car to get to work']\n",
      "\n",
      "Định nghĩa: a wheeled vehicle adapted to the rails of railroad\n",
      "Ví dụ: ['three cars had jumped the rails']\n",
      "\n",
      "Định nghĩa: the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant\n",
      "Ví dụ: []\n",
      "\n",
      "Định nghĩa: where passengers ride up and down\n",
      "Ví dụ: ['the car was on the top floor']\n",
      "\n",
      "Định nghĩa: a conveyance for passengers or freight on a cable railway\n",
      "Ví dụ: ['they took a cable car to the top of the mountain']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "word = \"car\"\n",
    "synsets = wordnet.synsets(word)\n",
    "\n",
    "for synset in synsets:\n",
    "    print(f\"Định nghĩa: {synset.definition()}\")\n",
    "    print(f\"Ví dụ: {synset.examples()}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đồng nghĩa: {'goodness', 'dear', 'practiced', 'in_effect', 'salutary', 'honest', 'adept', 'skilful', 'trade_good', 'right', 'proficient', 'estimable', 'sound', 'secure', 'serious', 'undecomposed', 'honorable', 'near', 'in_force', 'thoroughly', 'unspoilt', 'full', 'safe', 'just', 'skillful', 'effective', 'commodity', 'ripe', 'respectable', 'dependable', 'soundly', 'well', 'upright', 'expert', 'beneficial', 'unspoiled', 'good'}\n",
      "Trái nghĩa: {'badness', 'evilness', 'bad', 'evil', 'ill'}\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "        if lemma.antonyms():\n",
    "            antonyms.append(lemma.antonyms()[0].name())\n",
    "\n",
    "print(\"Đồng nghĩa:\", set(synonyms))\n",
    "print(\"Trái nghĩa:\", set(antonyms))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "sentence = word_tokenize(\"The quick brown fox jumps over the lazy dog.\")\n",
    "print(pos_tag(sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Độ giống nhau giữa 'car' và 'automobile': 1.0\n"
     ]
    }
   ],
   "source": [
    "word1 = wordnet.synsets(\"car\")[0]\n",
    "word2 = wordnet.synsets(\"automobile\")[0]\n",
    "\n",
    "similarity = word1.wup_similarity(word2)\n",
    "print(f\"Độ giống nhau giữa 'car' và 'automobile': {similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Độ giống nhau giữa 'run' và 'jog': 0.23529411764705882\n"
     ]
    }
   ],
   "source": [
    "verb1 = wordnet.synsets(\"run\")[0]\n",
    "verb2 = wordnet.synsets(\"jog\")[0]\n",
    "\n",
    "similarity = verb1.wup_similarity(verb2)\n",
    "print(f\"Độ giống nhau giữa 'run' và 'jog': {similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng tên nam: 2943\n",
      "Số lượng tên nữ: 5001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('names')\n",
    "from nltk.corpus import names\n",
    "\n",
    "male_names = names.words('male.txt')\n",
    "female_names = names.words('female.txt')\n",
    "\n",
    "print(f\"Số lượng tên nam: {len(male_names)}\")\n",
    "print(f\"Số lượng tên nữ: {len(female_names)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tên nam: ['Parke', 'Lamont', 'Keil', 'Wilfred', 'Teddie', 'Lanny', 'Lawson', 'Horatio', 'Esau', 'Jerald', 'Pincas', 'Jeremy', 'Garwin', 'Orion', 'Blake']\n",
      "Tên nữ: ['Alene', 'Katerine', 'Shena', 'Bettye', 'Fern', 'Gina', 'Cyndy', 'Nerti', 'Angela', 'Deonne', 'Florida', 'Lynsey', 'Zorah', 'Tella', 'Nicola']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random_male_names = random.sample(male_names, 15)\n",
    "random_female_names = random.sample(female_names, 15)\n",
    "\n",
    "print(\"Tên nam:\", random_male_names)\n",
    "print(\"Tên nữ:\", random_female_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 ký tự cuối của tên nam: ['r', 'n', 'y', 'e', 't', 't', 'y', 'l', 'l', 'm', 'h', 'e', 'l', 'd', 'r']\n",
      "15 ký tự cuối của tên nữ: ['l', 'l', 'e', 'y', 'i', 'e', 'y', 'l', 'l', 'e', 'a', 'a', 'a', 'h', 'e']\n"
     ]
    }
   ],
   "source": [
    "male_last_letters = [name[-1] for name in male_names]\n",
    "female_last_letters = [name[-1] for name in female_names]\n",
    "\n",
    "print(\"15 ký tự cuối của tên nam:\", male_last_letters[:15])\n",
    "print(\"15 ký tự cuối của tên nữ:\", female_last_letters[:15])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
